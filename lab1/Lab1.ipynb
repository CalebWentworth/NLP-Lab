{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kubuV0p00xOd"
   },
   "source": [
    "practice 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZiwPqon3la8"
   },
   "source": [
    "#1.1\n",
    "Odd numbers 0-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2_KxWMz4QdM"
   },
   "outputs": [],
   "source": [
    "for num in range(0, 100):\n",
    "    if num % 2 == 1:\n",
    "        print(num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSIytF5c5pzu"
   },
   "source": [
    "#1.2\n",
    "summation even numbers 0-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYTFFv4Z1duE",
    "outputId": "d8a0139e-500e-4bab-e40f-f12f03f5bfb7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summation(start, end):\n",
    "  result = 0\n",
    "  for i in range(start, end+1):\n",
    "    if i%2 == 0:\n",
    "      result+=i\n",
    "  return result\n",
    "\n",
    "print(summation(0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6HjgMg-3n-1",
    "tags": []
   },
   "source": [
    "#1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_Tqahca-8wh",
    "outputId": "72e72166-2a09-4e31-ab79-13b8bbcd1bf9"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "#import collections\n",
    "\n",
    "nums = []\n",
    "\n",
    "for i in range(10):\n",
    "  nums.append(random.randint(0,100))\n",
    "\n",
    "#max value\n",
    "print(max(nums))\n",
    "\n",
    "#min values\n",
    "print(min(nums))\n",
    "\n",
    "#sorted list\n",
    "nums = sorted(nums)\n",
    "print(nums)\n",
    "\n",
    "#shuffel list\n",
    "random.shuffle(nums)\n",
    "print(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBzrCzCm8eBc"
   },
   "source": [
    "#1.4\n",
    "Define a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eepTy9s58ki9",
    "outputId": "45c8810f-d356-4aec-806a-fa486b319d43"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "k_v = {'a': 1, 'd': 2, 'b': 2, 'c': 3}\n",
    "k_v = OrderedDict(k_v)\n",
    "\n",
    "print(k_v)\n",
    "k_v = OrderedDict(reversed(list(k_v.items())))\n",
    "newV = {'e' : 2}\n",
    "k_v.update(newV)\n",
    "\n",
    "print(k_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73meXCgvDhs4"
   },
   "source": [
    "#2.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#imports the corpus of stop words\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#chat gpt generated a regex to remove punctuation but keep hyphenated words\n",
    "#returns a list of items that match the regex\n",
    "def split_string(s):\n",
    "    return re.findall(r'\\b(?:[\\w*–-]+(?:-|\\*)?[\\w*–-]+)+\\b', s)\n",
    "\n",
    "#filters the list of words for those in stop_words\n",
    "def stopStrip(words):\n",
    "    nonStop = list(filter(lambda x: x not in stop_words, words))\n",
    "    return nonStop\n",
    "#does all the compearing between the file and string inputs.\n",
    "# calls the other functions\n",
    "def compare(s1, s2):\n",
    "    \n",
    "    words1 = split_string(s1)\n",
    "    words2 = split_string(s2)\n",
    "    \n",
    "    wordSet1 = set(words1)\n",
    "    wordSet2 = set(words2)\n",
    "    \n",
    "    freq1 = Counter(words1).most_common(1)\n",
    "    freq2 = Counter(words2).most_common(1)\n",
    "    \n",
    "    freqStop1 = Counter(stopStrip(words1)).most_common(1)\n",
    "    freqStop2 = Counter(stopStrip(words2)).most_common(1)\n",
    "                        \n",
    "    print(\"String input words: \" + str(len(words1)))\n",
    "    print(\"File input words: \" + str(len(words2)))\n",
    "    print(\"String input unique words: \" + str(len(wordSet1)))\n",
    "    print(\"File input unique words: \" + str(len(wordSet2)))\n",
    "    print(\"String input most common: \" + str(freq1))\n",
    "    print(\"File input most common: \" + str(freq2))\n",
    "    print(\"String input most common (-stop): \" + str(freqStop1))\n",
    "    print(\"File input most common (-stop): \" + str(freqStop2))\n",
    "    \n",
    "    return\n",
    "\n",
    "tfidf = \"\"\"In information retrieval, \n",
    "        tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), \n",
    "        short for term frequency–inverse document frequency, \n",
    "        is a numerical statistic that is intended to reflect how important \n",
    "        a word is to a document in a collection or corpus.[1] \n",
    "        It is often used as a weighting factor in searches of information\n",
    "        retrieval, text mining, and user modeling. \n",
    "        The tf–idf value increases proportionally to the number of times \n",
    "        a word appears in the document and is offset by the number of documents\n",
    "        in the corpus that contain the word, which helps to adjust for the fact\n",
    "        that some words appear more frequently in general. \"\"\"\n",
    "\n",
    "contents = open('TFIDF.txt', 'r').read()\n",
    "\n",
    "compare(tfidf, contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "{'Author': 'Arthur Conan Doyle', 'Size': 100}\n",
      "Agatha Christie\n",
      "Alexandre Dumas\n",
      "Alexandre Dumas\n",
      "Alexandre Dumas\n",
      "Anton Chekhov\n",
      "Arthur Conan Doyle\n",
      "Arthur Conan Doyle\n",
      "Arthur Conan Doyle\n",
      "[('the', 44), ('is', 35), ('to', 31), ('I', 25), ('a', 25)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "quotes = pd.read_csv('quotes.tsv',sep='\\t', header=0)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "#checks every quote's author adds 1 if it matches alexandre dumas\n",
    "def alex_quotes():\n",
    "    alex_quotes = 0\n",
    "    for index, row in quotes.iterrows() :\n",
    "      if row['Author'] == 'Alexandre Dumas':\n",
    "        alex_quotes += 1\n",
    "    print(alex_quotes)\n",
    "    return\n",
    "\n",
    "\n",
    "#tokenizes each quote, strips punctuation marks, compears to the previous longest quote\n",
    "def quoteLen():\n",
    "    longestQuote = {'Author' : '' ,'Size' : 0}\n",
    "    \n",
    "    for index, row in quotes.iterrows() :\n",
    "        sentance = row['Quote']\n",
    "        author = row['Author']\n",
    "        words = nltk.tokenize.word_tokenize(sentance,\"english\", False)\n",
    "        words = [s for s in words if not any(c in punctuation for c in s)]\n",
    "        if len(words) > longestQuote.get('Size'):\n",
    "            longestQuote['Author']=author\n",
    "            longestQuote['Size']=len(words)\n",
    "            \n",
    "    return (longestQuote)\n",
    "\n",
    "#tokenizes each quote and looks for a matching token prints the authors name.\n",
    "def oneAuthor():\n",
    "    for index, row in quotes.iterrows() :\n",
    "        sentance = row['Quote']\n",
    "        author = row['Author']\n",
    "        words = nltk.tokenize.word_tokenize(sentance,\"english\", False)\n",
    "        if \"one\" in words:\n",
    "            print(author)\n",
    "    return (\"not found\")\n",
    "\n",
    "#this function tokenizes each quote by word, strips out punctuation\n",
    "#it then adds that quote's tokens to a super list of every quote's tokens\n",
    "#then it generates the frequency using Counter\n",
    "#tokens with the same hash have their frequency increased the 5 most common are printed\n",
    "def frequency():\n",
    "    quotesTotal = []\n",
    "    for index, row in quotes.iterrows() :\n",
    "        sentance = row['Quote']\n",
    "        words = nltk.tokenize.word_tokenize(sentance,\"english\", False)\n",
    "        words = [s for s in words if not any(c in punctuation for c in s)]\n",
    "        quotesTotal += words\n",
    "    frequent = Counter(quotesTotal).most_common(5)\n",
    "    return (frequent)\n",
    "\n",
    "alex_quotes()\n",
    "print(quoteLen())\n",
    "oneAuthor()\n",
    "print(frequency())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
